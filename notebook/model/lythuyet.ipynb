{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b2ecc3",
   "metadata": {},
   "source": [
    "# Tendency\n",
    "\n",
    "**Tendency** là một đặc trưng biểu diễn xu hướng thay đổi của nhịp tim thai nhi theo thời gian. Giá trị của Tendency gồm `-1`, `0` và `1`, lần lượt tương ứng với các trạng thái **giảm**, **không đổi**, và **tăng**.\n",
    "\n",
    "- Tendency là biến phân loại có thứ tự (Ordinal Categorical Feature), vì các giá trị mang ý nghĩa thứ tự: `giảm < không đổi < tăng`.\n",
    "- Trong tiền xử lý, thay vì dùng **one-hot encoding** (mất thông tin thứ tự), ta giữ nguyên các giá trị `-1`, `0`, `1`.\n",
    "- Để đồng nhất thang đo giữa các đặc trưng, Tendency được **chuẩn hóa** cùng với các đặc trưng liên tục khác bằng:\n",
    "  - `StandardScaler`\n",
    "  - hoặc `MinMaxScaler`\n",
    "- Việc chuẩn hóa giúp mô hình đánh giá đúng mức độ ảnh hưởng của từng đặc trưng, không bị lệch bởi đơn vị đo khác nhau.\n",
    "\n",
    "\n",
    "# Nhãn có thứ tự\n",
    "\n",
    "## Khoảng cách trong KNN\n",
    "\n",
    "- Nhãn `NSP` là dữ liệu có thứ tự:  \n",
    "  `1: Bình thường`, `2: Nghi ngờ`, `3: Bệnh lý`.\n",
    "- Nên sử dụng:\n",
    "  - **Euclidean distance**: phù hợp nếu các đặc trưng cùng đơn vị.\n",
    "  - **Manhattan distance**: tốt hơn khi dữ liệu có phân bố không đều hoặc nhiều giá trị cực đoan.\n",
    "\n",
    "## Hạn chế của Accuracy\n",
    "\n",
    "- **Accuracy không phân biệt lỗi nặng hay nhẹ**:\n",
    "  - Dự đoán sai `1 -> 3` nguy hiểm hơn `1 -> 2`, nhưng Accuracy coi như nhau.\n",
    "\n",
    "### Nên dùng các chỉ số sau:\n",
    "\n",
    "#### 1. MAE (Mean Absolute Error)\n",
    "- Đo khoảng cách sai lệch giữa nhãn thực và dự đoán.\n",
    "- Ví dụ:\n",
    "  - `Dự đoán 1 -> 3`: MAE = |1 - 3| = 2 (lỗi nặng)\n",
    "  - `Dự đoán 1 -> 2`: MAE = |1 - 2| = 1 (lỗi nhẹ)\n",
    "- **MAE càng nhỏ càng tốt.**\n",
    "\n",
    "#### 2. Weighted F1-Score\n",
    "- Đánh giá riêng từng lớp, có trọng số theo tỷ lệ dữ liệu.\n",
    "- Phát hiện tình trạng mô hình bỏ qua lớp thiểu số.\n",
    "\n",
    "### Ví dụ:\n",
    "\n",
    "| Mô hình   | Accuracy | MAE  |\n",
    "|-----------|----------|------|\n",
    "| A         | 90%      | 0.5  |\n",
    "| B         | 90%      | 1.2  |\n",
    "\n",
    "=> Mô hình A tốt hơn vì ít mắc lỗi nghiêm trọng hơn.\n",
    "\n",
    "### Tóm lại:\n",
    "\n",
    "- **MAE**: Tránh lỗi chẩn đoán nguy hiểm.\n",
    "- **F1-Score**: Đảm bảo phát hiện đủ các mức độ.\n",
    "\n",
    "> Kết hợp cả hai chỉ số để đánh giá đúng chất lượng mô hình.\n",
    "\n",
    "\n",
    "\n",
    "# Softmax vs Ordinal Regression\n",
    "\n",
    "- **Softmax** (Multinomial Logistic Regression):  \n",
    "  => Coi các lớp là không có thứ tự => Không phù hợp với NSP.\n",
    "- Nhược điểm:\n",
    "  - Không phân biệt lỗi `1 -> 2` với `1 -> 3`.\n",
    "- **Giải pháp**: Dùng **Ordinal Logistic Regression** để tận dụng thông tin thứ tự giữa các lớp.\n",
    "\n",
    "\n",
    "# Mô hình SVM\n",
    "\n",
    "## 1. Giới thiệu về SVM\n",
    "- SVM (Support Vector Machine): mô hình học có giám sát, dùng cho phân loại và hồi quy.\n",
    "- Ý tưởng: tìm **siêu phẳng (hyperplane)** phân chia hai lớp sao cho **khoảng cách (margin)** tới các điểm gần nhất là lớn nhất.\n",
    "\n",
    "## 2. Margin (Khoảng cách)\n",
    "- Là khoảng cách từ siêu phẳng đến các điểm gần nhất.\n",
    "- Các điểm này gọi là **support vectors**.\n",
    "- Mục tiêu: **tối đa hóa margin** để tránh overfitting.\n",
    "\n",
    "## 3. SVM tuyến tính\n",
    "\n",
    "### 3.1 Mô tả dữ liệu\n",
    "- Dữ liệu huấn luyện: (xᵢ, yᵢ) với xᵢ ∈ ℝᵈ, yᵢ ∈ {-1, 1}.\n",
    "\n",
    "### 3.2 Mô hình siêu phẳng\n",
    "- Phương trình: `wᵗx + b = 0`\n",
    "  - `w`: vector vuông góc với siêu phẳng\n",
    "  - `b`: độ dịch chuyển\n",
    "\n",
    "### 3.3 Ràng buộc phân loại\n",
    "- Mỗi điểm phải thỏa: `yᵢ(wᵗxᵢ + b) ≥ 1`\n",
    "\n",
    "### 3.4 Bài toán tối ưu\n",
    "- Tối ưu: `min (1/2)||w||²`\n",
    "- Với ràng buộc: `yᵢ(wᵗxᵢ + b) ≥ 1 ∀ i`\n",
    "\n",
    "### 3.5 Giải bằng Lagrangian\n",
    "- L(w, b, α) = (1/2)||w||² − Σαᵢ[yᵢ(wᵗxᵢ + b) − 1]\n",
    "- αᵢ ≥ 0: hệ số Lagrange.\n",
    "- Giải bài toán đối ngẫu (dual).\n",
    "\n",
    "## 4. Soft Margin SVM\n",
    "- Khi dữ liệu không phân cách được hoàn toàn, thêm biến **slack ξᵢ ≥ 0**:\n",
    "  - Ràng buộc: `yᵢ(wᵗxᵢ + b) ≥ 1 − ξᵢ`\n",
    "- Bài toán tối ưu:\n",
    "  - `min (1/2)||w||² + C Σξᵢ`\n",
    "  - `C`: siêu tham số cân bằng giữa margin lớn và ít vi phạm.\n",
    "\n",
    "## 5. Kernel Trick (phi tuyến)\n",
    "- Ánh xạ dữ liệu lên không gian đặc trưng cao hơn.\n",
    "- Không cần ánh xạ tường minh nhờ **kernel function**:\n",
    "  - `K(x, x') = φ(x)ᵗφ(x')`\n",
    "\n",
    "### 5.1 Các hàm kernel phổ biến\n",
    "- **Linear**: `K(x, x') = xᵗx'`\n",
    "- **Polynomial**: `K(x, x') = (xᵗx' + c)^d`\n",
    "- **Gaussian (RBF)**: `K(x, x') = exp(-||x − x'||² / 2σ²)`\n",
    "\n",
    "## 6. Tóm tắt\n",
    "- SVM mạnh nhờ tối ưu khoảng cách.\n",
    "- **Kernel trick** xử lý dữ liệu phi tuyến hiệu quả.\n",
    "- **Soft margin** cho phép mô hình chịu được dữ liệu không hoàn hảo.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
