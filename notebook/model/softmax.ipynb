{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92fb3913",
   "metadata": {},
   "source": [
    "# Há»’I QUY SOFTMAX (MULTINOMIAL LOGISTIC REGRESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e570b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6183ec37",
   "metadata": {},
   "source": [
    "## HÃ m softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "966cffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels(y, C):\n",
    "    Y = sparse.coo_matrix((np.ones_like(y), (y, np.arange(len(y)))), shape=(C, len(y))).toarray()\n",
    "    return Y\n",
    "\n",
    "def softmax_stable(Z):\n",
    "    e_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    return e_Z / e_Z.sum(axis=0)\n",
    "\n",
    "def softmax(Z):\n",
    "    e_Z = np.exp(Z)\n",
    "    return e_Z / e_Z.sum(axis=0)\n",
    "\n",
    "def softmax_regression(X, y, W_init, eta=0.05, tol=1e-4, max_count=10000):\n",
    "    W = [W_init]\n",
    "    C = W_init.shape[1]\n",
    "    Y = convert_labels(y, C)\n",
    "    N = X.shape[1]\n",
    "    d = X.shape[0]\n",
    "\n",
    "    count = 0\n",
    "    check_w_after = 1500\n",
    "    while count < max_count:\n",
    "        mix_id = np.random.permutation(N)\n",
    "        for i in mix_id:\n",
    "            xi = X[:, i].reshape(d, 1)\n",
    "            yi = Y[:, i].reshape(C, 1)\n",
    "            ai = softmax(np.dot(W[-1].T, xi))\n",
    "            W_new = W[-1] + eta * xi.dot((yi - ai).T)\n",
    "            count += 1\n",
    "            if count % check_w_after == 0:\n",
    "                if np.linalg.norm(W_new - W[-check_w_after]) < tol:\n",
    "                    return W\n",
    "            W.append(W_new)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33c63ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dá»± Ä‘oÃ¡n vÃ  Ä‘Ã¡nh giÃ¡ cÃ¡c chá»‰ sá»‘ \n",
    "def pred(W, X):\n",
    "    A = softmax_stable(W.T.dot(X))\n",
    "    return np.argmax(A, axis=0)\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    auc = roc_auc_score(pd.get_dummies(y_true), pd.get_dummies(y_pred), average='macro', multi_class='ovr')\n",
    "    print(f\"AUC (macro average): {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9654d3e3",
   "metadata": {},
   "source": [
    "## Dá»® LIá»†U Gá»C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06f8be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dá»¯ liá»‡u\n",
    "df = pd.read_csv(\"../../data/data_processed/data_processed.csv\")\n",
    "\n",
    "# TÃ¡ch Ä‘áº·c trÆ°ng vÃ  nhÃ£n\n",
    "X = df.drop(columns=[\"NSP\"]).values\n",
    "y = df[\"NSP\"].values - 1  # Chuyá»ƒn vá» 0,1,2\n",
    "\n",
    "# Chuáº©n hÃ³a\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ThÃªm bias\n",
    "X_scaled = np.hstack([np.ones((X_scaled.shape[0], 1)), X_scaled])\n",
    "C = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd137770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tá»‰ lá»‡ train:test = 8:2 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.94      0.93       332\n",
      "         1.0       0.58      0.59      0.59        59\n",
      "         2.0       0.82      0.66      0.73        35\n",
      "\n",
      "    accuracy                           0.87       426\n",
      "   macro avg       0.78      0.73      0.75       426\n",
      "weighted avg       0.87      0.87      0.87       426\n",
      "\n",
      "Confusion Matrix:\n",
      "[[312  17   3]\n",
      " [ 22  35   2]\n",
      " [  4   8  23]]\n",
      "AUC (macro average): 0.8054\n",
      "\n",
      "--- Tá»‰ lá»‡ train:test = 7:3 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.91      0.93       497\n",
      "         1.0       0.50      0.72      0.59        88\n",
      "         2.0       0.83      0.55      0.66        53\n",
      "\n",
      "    accuracy                           0.86       638\n",
      "   macro avg       0.76      0.73      0.73       638\n",
      "weighted avg       0.88      0.86      0.86       638\n",
      "\n",
      "Confusion Matrix:\n",
      "[[454  40   3]\n",
      " [ 22  63   3]\n",
      " [  2  22  29]]\n",
      "AUC (macro average): 0.8139\n",
      "\n",
      "--- Tá»‰ lá»‡ train:test = 6:4 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.94      0.94       663\n",
      "         1.0       0.62      0.65      0.63       118\n",
      "         2.0       0.79      0.70      0.74        70\n",
      "\n",
      "    accuracy                           0.88       851\n",
      "   macro avg       0.78      0.77      0.77       851\n",
      "weighted avg       0.88      0.88      0.88       851\n",
      "\n",
      "Confusion Matrix:\n",
      "[[626  35   2]\n",
      " [ 30  77  11]\n",
      " [  8  13  49]]\n",
      "AUC (macro average): 0.8354\n"
     ]
    }
   ],
   "source": [
    "# HÃ m huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ vá»›i tá»‰ lá»‡ cho trÆ°á»›c\n",
    "def train_and_evaluate(X, y, test_size):\n",
    "    print(f\"\\n--- Tá»‰ lá»‡ train:test = {int((1-test_size)*10)}:{int(test_size*10)} ---\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n",
    "    X_train_T = X_train.T\n",
    "    X_test_T = X_test.T\n",
    "    d = X_train_T.shape[0]\n",
    "\n",
    "    W_init = np.random.randn(d, C)\n",
    "    W = softmax_regression(X_train_T, y_train, W_init)[-1]\n",
    "\n",
    "    y_pred = pred(W, X_test_T)\n",
    "    evaluate_model(y_test, y_pred)\n",
    "\n",
    "# Cháº¡y vá»›i cÃ¡c tá»‰ lá»‡\n",
    "for test_size in [0.2, 0.3, 0.4]:\n",
    "    train_and_evaluate(X_scaled, y, test_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7b2309",
   "metadata": {},
   "source": [
    "### ÄÃ¡nh giÃ¡ tá»•ng quan theo Ä‘á»™ chÃ­nh xÃ¡c vÃ  AUC\n",
    "\n",
    "| Tá»‰ lá»‡ train:test | Accuracy | AUC (macro) | Macro F1-score |\n",
    "|------------------|----------|-------------|----------------|\n",
    "| 8:2              | 0.8779   | 0.9568      | 0.7665         |\n",
    "| 7:3              | 0.8840   | 0.9545      | 0.7609         |\n",
    "| 6:4              | **0.8931** | **0.9631**    | **0.7898**       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b45b6",
   "metadata": {},
   "source": [
    "**Lá»›p 0 (bÃ¬nh thÆ°á»ng):**\n",
    "Precision vÃ  Recall Ä‘á»u ráº¥t cao á»Ÿ cáº£ 3 tá»‰ lá»‡ (trÃªn 92%).\n",
    "\n",
    "ÄÃ¢y lÃ  lá»›p chiáº¿m Ä‘a sá»‘ nÃªn mÃ´ hÃ¬nh há»c tá»‘t nháº¥t.\n",
    "\n",
    "**Lá»›p 1 (nghi ngá»):**\n",
    "Hiá»‡u suáº¥t biáº¿n Ä‘á»™ng rÃµ rá»‡t, Precision/Recall dao Ä‘á»™ng trong khoáº£ng 59â€“68%.\n",
    "\n",
    "Tá»‰ lá»‡ 6:4 cho káº¿t quáº£ á»•n Ä‘á»‹nh hÆ¡n vá» F1 (0.63), dÃ¹ Recall hÆ¡i giáº£m nháº¹.\n",
    "\n",
    "**Lá»›p 2 (báº¥t thÆ°á»ng):**\n",
    "Nháº¡y cáº£m vá»›i sá»± thay Ä‘á»•i tá»‰ lá»‡ chia.\n",
    "\n",
    "Tá»‰ lá»‡ 6:4 cÃ³ Precision 85% vÃ  F1-score gáº§n 0.79 â€” tá»‘t nháº¥t trong 3 lá»±a chá»n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf4588a",
   "metadata": {},
   "source": [
    "Lá»›p 1 (suspect) thÆ°á»ng cÃ³ precision vÃ  recall tháº¥p do Ä‘áº·c Ä‘iá»ƒm Ä‘áº·c trÆ°ng cá»§a nÃ³ náº±m giá»¯a hai lá»›p cÃ²n láº¡i (normal vÃ  pathologic), khiáº¿n mÃ´ hÃ¬nh dá»… nháº§m láº«n. ÄÃ¢y lÃ  lá»›p trung gian vá» máº·t y há»c, nÃªn cÃ¡c Ä‘áº·c trÆ°ng khÃ´ng phÃ¢n tÃ¡ch rÃµ rÃ ng, dáº«n Ä‘áº¿n viá»‡c mÃ´ hÃ¬nh thÆ°á»ng dá»± Ä‘oÃ¡n sai sang lá»›p 0 hoáº·c lá»›p 2. Äá»“ng thá»i, softmax regression lÃ  mÃ´ hÃ¬nh tuyáº¿n tÃ­nh nÃªn cÃ ng gáº·p khÃ³ khÄƒn khi ranh giá»›i giá»¯a cÃ¡c lá»›p khÃ´ng rÃµ rÃ ng hoáº·c cÃ³ sá»± chá»“ng láº¥n trong khÃ´ng gian Ä‘áº·c trÆ°ng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c099a",
   "metadata": {},
   "source": [
    "## Dá»® LIá»†U GIáº¢M CHIá»€U "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7489ce84",
   "metadata": {},
   "source": [
    "### GIáº¢M TRÆ¯á»šC CHIA SAU "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c934533",
   "metadata": {},
   "source": [
    "### PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "309331cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ÄÃ¡nh giÃ¡ vá»›i PCA ---\n",
      "\n",
      "--- Tá»‰ lá»‡ train:test = 8:2 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.78      0.87       332\n",
      "         1.0       0.44      0.88      0.58        59\n",
      "         2.0       0.43      0.51      0.47        35\n",
      "\n",
      "    accuracy                           0.77       426\n",
      "   macro avg       0.62      0.73      0.64       426\n",
      "weighted avg       0.86      0.77      0.80       426\n",
      "\n",
      "Confusion Matrix:\n",
      "[[260  51  21]\n",
      " [  4  52   3]\n",
      " [  1  16  18]]\n",
      "AUC (macro average): 0.8136\n",
      "\n",
      "--- Tá»‰ lá»‡ train:test = 7:3 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.79      0.87       497\n",
      "         1.0       0.45      0.78      0.57        88\n",
      "         2.0       0.39      0.57      0.47        53\n",
      "\n",
      "    accuracy                           0.77       638\n",
      "   macro avg       0.60      0.71      0.64       638\n",
      "weighted avg       0.85      0.77      0.80       638\n",
      "\n",
      "Confusion Matrix:\n",
      "[[395  61  41]\n",
      " [ 14  69   5]\n",
      " [  0  23  30]]\n",
      "AUC (macro average): 0.8024\n",
      "\n",
      "--- Tá»‰ lá»‡ train:test = 6:4 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.80      0.88       663\n",
      "         1.0       0.51      0.74      0.60       118\n",
      "         2.0       0.40      0.79      0.53        70\n",
      "\n",
      "    accuracy                           0.79       851\n",
      "   macro avg       0.63      0.77      0.67       851\n",
      "weighted avg       0.86      0.79      0.81       851\n",
      "\n",
      "Confusion Matrix:\n",
      "[[529  69  65]\n",
      " [ 13  87  18]\n",
      " [  0  15  55]]\n",
      "AUC (macro average): 0.8385\n"
     ]
    }
   ],
   "source": [
    "pca_df = pd.read_csv(\"../../data/dimension_reduction/pca/pca_all.csv\")\n",
    "pca_X = pca_df.drop(columns=[\"NSP\"]).values\n",
    "pca_y = pca_df[\"NSP\"].values - 1\n",
    "scaler = StandardScaler()\n",
    "pca_X_scaled = scaler.fit_transform(pca_X)\n",
    "\n",
    "print(\"--- ÄÃ¡nh giÃ¡ vá»›i PCA ---\")\n",
    "for test_size in [0.2, 0.3, 0.4]:\n",
    "    train_and_evaluate(pca_X_scaled, pca_y, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ddb2d0",
   "metadata": {},
   "source": [
    "### Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vá»›i PCA\n",
    "\n",
    "| Tá»‰ lá»‡ chia | Accuracy | AUC (macro) | F1-score (macro) |\n",
    "|------------|----------|-------------|------------------|\n",
    "| 8:2        | 0.7606   | 0.8786      | 0.6042           |\n",
    "| 7:3        | **0.7759**   | 0.8740      | 0.6356           |\n",
    "| 6:4        | 0.7591   | **0.9192**  | **0.6380**       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8974105",
   "metadata": {},
   "source": [
    "### LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0d0da5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ÄÃ¡nh giÃ¡ vá»›i LDA ---\n",
      "\n",
      "--- Tá»‰ lá»‡ train:test = 8:2 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.77      0.86       332\n",
      "         1.0       0.47      0.80      0.59        59\n",
      "         2.0       0.37      0.66      0.47        35\n",
      "\n",
      "    accuracy                           0.77       426\n",
      "   macro avg       0.60      0.74      0.64       426\n",
      "weighted avg       0.86      0.77      0.79       426\n",
      "\n",
      "Confusion Matrix:\n",
      "[[257  42  33]\n",
      " [  6  47   6]\n",
      " [  0  12  23]]\n",
      "AUC (macro average): 0.8195\n",
      "\n",
      "--- Tá»‰ lá»‡ train:test = 7:3 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.76      0.86       497\n",
      "         1.0       0.44      0.92      0.59        88\n",
      "         2.0       0.42      0.57      0.48        53\n",
      "\n",
      "    accuracy                           0.77       638\n",
      "   macro avg       0.62      0.75      0.65       638\n",
      "weighted avg       0.87      0.77      0.79       638\n",
      "\n",
      "Confusion Matrix:\n",
      "[[378  82  37]\n",
      " [  3  81   4]\n",
      " [  0  23  30]]\n",
      "AUC (macro average): 0.8275\n",
      "\n",
      "--- Tá»‰ lá»‡ train:test = 6:4 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.78      0.87       663\n",
      "         1.0       0.46      0.88      0.60       118\n",
      "         2.0       0.43      0.59      0.49        70\n",
      "\n",
      "    accuracy                           0.78       851\n",
      "   macro avg       0.62      0.75      0.66       851\n",
      "weighted avg       0.86      0.78      0.80       851\n",
      "\n",
      "Confusion Matrix:\n",
      "[[518  94  51]\n",
      " [ 10 104   4]\n",
      " [  0  29  41]]\n",
      "AUC (macro average): 0.8262\n"
     ]
    }
   ],
   "source": [
    "lda_df = pd.read_csv(\"../../data/dimension_reduction/lda/lda_all.csv\")\n",
    "\n",
    "lda_X = lda_df.drop(columns=[\"NSP\"]).values\n",
    "lda_y = lda_df[\"NSP\"].values - 1\n",
    "lda_X_scaled = scaler.fit_transform(lda_X)\n",
    "\n",
    "print(\"\\n--- ÄÃ¡nh giÃ¡ vá»›i LDA ---\")\n",
    "for test_size in [0.2, 0.3, 0.4]:\n",
    "  train_and_evaluate(lda_X_scaled, lda_y, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2725a18",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vá»›i LDA\n",
    "\n",
    "| Tá»‰ lá»‡ chia | Accuracy | AUC (macro) | F1-score (macro) |\n",
    "|------------|----------|-------------|------------------|\n",
    "| 8:2        | 0.7700   | 0.9112      | 0.6445           |\n",
    "| 7:3        | 0.7633   | 0.8996      | 0.6356           |\n",
    "| 6:4        | **0.7779** | **0.9294** | **0.6613**       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5cd5ce",
   "metadata": {},
   "source": [
    "### So sÃ¡nh giá»¯a PCA vÃ  LDA:\n",
    "PCA: Äáº¡t káº¿t quáº£ tá»‘t nháº¥t cho lá»›p 0, nhÆ°ng vá»›i cÃ¡c lá»›p 1 vÃ  2, precision vÃ  recall tháº¥p, cÃ³ thá»ƒ vÃ¬ PCA chá»§ yáº¿u tá»‘i Æ°u hÃ³a viá»‡c giáº£m chiá»u mÃ  khÃ´ng chÃº Ã½ Ä‘áº¿n sá»± phÃ¢n biá»‡t giá»¯a cÃ¡c lá»›p. AUC cÃ³ pháº§n tháº¥p hÆ¡n so vá»›i LDA.\n",
    "\n",
    "LDA: Máº·c dÃ¹ precision cá»§a lá»›p 1 vÃ  2 váº«n khÃ´ng quÃ¡ cao, nhÆ°ng recall vÃ  AUC cho tháº¥y LDA cÃ³ kháº£ nÄƒng phÃ¢n biá»‡t cÃ¡c lá»›p tá»‘t hÆ¡n, Ä‘áº·c biá»‡t trong cÃ¡c tá»‰ lá»‡ chia nhá» hÆ¡n. LDA cÃ³ xu hÆ°á»›ng cáº£i thiá»‡n kháº£ nÄƒng phÃ¢n loáº¡i cho cÃ¡c lá»›p nhá» hÆ¡n (lá»›p 1 vÃ  2), trong khi PCA váº«n táº­p trung vÃ o lá»›p 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b698960",
   "metadata": {},
   "source": [
    "## CHIA TRÆ¯á»šC GIáº¢M SAU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0598dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_from_file(train_path, test_path, test_ratio_label):\n",
    "    print(f\"\\n--- Tá»‰ lá»‡ train:test = {test_ratio_label} ---\")\n",
    "    \n",
    "    # Äá»c dá»¯ liá»‡u\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    X_train = train_df.drop(columns=['Unnamed: 0', \"NSP\"], axis=1).values.T\n",
    "    y_train = train_df['NSP'].values - 1\n",
    "    X_test = test_df.drop(columns=['Unnamed: 0', \"NSP\"], axis=1).values.T\n",
    "    y_test = test_df['NSP'].values - 1\n",
    "\n",
    "    # Huáº¥n luyá»‡n\n",
    "    d = X_train.shape[0]\n",
    "    W_init = np.random.randn(d, C)\n",
    "    W = softmax_regression(X_train, y_train, W_init)[-1]\n",
    "\n",
    "    # Dá»± Ä‘oÃ¡n vÃ  Ä‘Ã¡nh giÃ¡\n",
    "    y_pred = pred(W, X_test)\n",
    "    evaluate_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef744904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tá»‰ lá»‡ train:test = 8:2 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.74      0.84       332\n",
      "         1.0       0.37      0.83      0.51        59\n",
      "         2.0       0.36      0.46      0.41        35\n",
      "\n",
      "    accuracy                           0.73       426\n",
      "   macro avg       0.57      0.68      0.59       426\n",
      "weighted avg       0.85      0.73      0.76       426\n",
      "\n",
      "Confusion Matrix:\n",
      "[[245  67  20]\n",
      " [  2  49   8]\n",
      " [  3  16  16]]\n",
      "AUC (macro average): 0.7791\n",
      "\n",
      "--- Tá»‰ lá»‡ train:test = 7:3 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.76      0.85       497\n",
      "         1.0       0.41      0.83      0.54        88\n",
      "         2.0       0.41      0.57      0.48        53\n",
      "\n",
      "    accuracy                           0.75       638\n",
      "   macro avg       0.60      0.72      0.63       638\n",
      "weighted avg       0.85      0.75      0.78       638\n",
      "\n",
      "Confusion Matrix:\n",
      "[[377  84  36]\n",
      " [  8  73   7]\n",
      " [  0  23  30]]\n",
      "AUC (macro average): 0.8049\n",
      "\n",
      "--- Tá»‰ lá»‡ train:test = 6:4 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.81      0.88       663\n",
      "         1.0       0.29      0.31      0.30       118\n",
      "         2.0       0.31      0.71      0.43        70\n",
      "\n",
      "    accuracy                           0.74       851\n",
      "   macro avg       0.52      0.61      0.54       851\n",
      "weighted avg       0.81      0.74      0.76       851\n",
      "\n",
      "Confusion Matrix:\n",
      "[[540  72  51]\n",
      " [ 21  37  60]\n",
      " [  1  19  50]]\n",
      "AUC (macro average): 0.7432\n"
     ]
    }
   ],
   "source": [
    "pca_files = [\n",
    "    (\"../../data/dimension_reduction/pca/train_80.csv\", \"../../data/dimension_reduction/pca/test_20.csv\", \"8:2\"),\n",
    "    (\"../../data/dimension_reduction/pca/train_70.csv\", \"../../data/dimension_reduction/pca/test_30.csv\", \"7:3\"),\n",
    "    (\"../../data/dimension_reduction/pca/train_60.csv\", \"../../data/dimension_reduction/pca/test_40.csv\", \"6:4\"),\n",
    "]\n",
    "\n",
    "for train_file, test_file, label in pca_files:\n",
    "    train_test_from_file(train_file, test_file, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aaa891cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tá»‰ lá»‡ train:test = 8:2 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.79      0.87       332\n",
      "         1.0       0.42      0.32      0.37        59\n",
      "         2.0       0.31      0.97      0.48        35\n",
      "\n",
      "    accuracy                           0.74       426\n",
      "   macro avg       0.57      0.69      0.57       426\n",
      "weighted avg       0.83      0.74      0.76       426\n",
      "\n",
      "Confusion Matrix:\n",
      "[[262  25  45]\n",
      " [ 11  19  29]\n",
      " [  0   1  34]]\n",
      "AUC (macro average): 0.7842\n",
      "\n",
      "--- Tá»‰ lá»‡ train:test = 7:3 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.78      0.87       497\n",
      "         1.0       0.43      0.83      0.56        88\n",
      "         2.0       0.43      0.57      0.49        53\n",
      "\n",
      "    accuracy                           0.77       638\n",
      "   macro avg       0.61      0.72      0.64       638\n",
      "weighted avg       0.85      0.77      0.79       638\n",
      "\n",
      "Confusion Matrix:\n",
      "[[387  75  35]\n",
      " [ 10  73   5]\n",
      " [  0  23  30]]\n",
      "AUC (macro average): 0.8095\n",
      "\n",
      "--- Tá»‰ lá»‡ train:test = 6:4 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.78      0.87       663\n",
      "         1.0       0.47      0.61      0.53       118\n",
      "         2.0       0.37      0.90      0.53        70\n",
      "\n",
      "    accuracy                           0.77       851\n",
      "   macro avg       0.61      0.76      0.64       851\n",
      "weighted avg       0.86      0.77      0.79       851\n",
      "\n",
      "Confusion Matrix:\n",
      "[[517  73  73]\n",
      " [ 13  72  33]\n",
      " [  0   7  63]]\n",
      "AUC (macro average): 0.8293\n"
     ]
    }
   ],
   "source": [
    "lda_files = [\n",
    "    (\"../../data/dimension_reduction/lda/train_80.csv\", \"../../data/dimension_reduction/lda/test_20.csv\", \"8:2\"),\n",
    "    (\"../../data/dimension_reduction/lda/train_70.csv\", \"../../data/dimension_reduction/lda/test_30.csv\", \"7:3\"),\n",
    "    (\"../../data/dimension_reduction/lda/train_60.csv\", \"../../data/dimension_reduction/lda/test_40.csv\", \"6:4\")\n",
    "]\n",
    "\n",
    "for train_file, test_file, label in lda_files:\n",
    "    train_test_from_file(train_file, test_file, label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
